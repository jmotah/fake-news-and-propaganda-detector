{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import model_utils \n",
    "\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertModel\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "\n",
    "TEST_PCT = 0.1\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "EPSILON = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZE AND CONCAT ALL DATASETS\n",
    "\n",
    "normalized_dfs = [\n",
    "    # Misinfo Dataset\n",
    "    model_utils.normalize_misinfo_dataset(\"datasets/misinfo-dataset/DataSet_Misinfo_FAKE.csv\"),\n",
    "    model_utils.normalize_misinfo_dataset(\"datasets/misinfo-dataset/DataSet_Misinfo_TRUE.csv\"),\n",
    "    model_utils.normalize_misinfo_dataset(\"datasets/misinfo-dataset/EXTRA_RussianPropagandaSubset.csv\"),\n",
    "    # Fake News Net Dataset\n",
    "    model_utils.normalize_fakenewsnet_dataset(\"datasets/fakenewsnet-dataset/gossipcop_fake.csv\"),\n",
    "    model_utils.normalize_fakenewsnet_dataset(\"datasets/fakenewsnet-dataset/gossipcop_real.csv\"),\n",
    "    model_utils.normalize_fakenewsnet_dataset(\"datasets/fakenewsnet-dataset/politifact_fake.csv\"),\n",
    "    model_utils.normalize_fakenewsnet_dataset(\"datasets/fakenewsnet-dataset/politifact_real.csv\"),\n",
    "    # Liar Dataset\n",
    "    model_utils.normalize_liar_dataset(\"datasets/liar-dataset/train.tsv\")\n",
    "]\n",
    "\n",
    "df = pd.concat(normalized_dfs, ignore_index=True)\n",
    "df = df.dropna(subset=[\"text\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DATALOADERS\n",
    "\n",
    "def make_data_loader(df: pd.DataFrame, tokenizer, batch_size: int = BATCH_SIZE, test_pct: float = TEST_PCT) -> tuple:\n",
    "    \"\"\"\n",
    "    Converts a dataframe of text and label columns into training and test dataloaders\n",
    "    \n",
    "    Parameters:\n",
    "        df: The pandas DataFrame of text and label columns\n",
    "        tokenizer: The Hugging-Face tokenizer to be used with our model\n",
    "        batch_size: The number of input sequences processed\n",
    "        test_pct: The percentage of data to be reserved for testing\n",
    "    \"\"\"\n",
    "    \n",
    "    # Grab features and label columns\n",
    "    X = df[\"text\"].tolist()\n",
    "    y = df[\"label\"].astype(int).tolist()\n",
    "    \n",
    "    # Tokenize \n",
    "    encoder = tokenizer(X, truncation=True, return_tensors='pt', padding=\"max_length\", max_length=MAX_LENGTH)\n",
    "    \n",
    "    # Wrap into TensorDataset\n",
    "    dataset = TensorDataset(encoder[\"input_ids\"], encoder[\"attention_mask\"], torch.tensor(y))\n",
    "    \n",
    "    # Train/Test Split\n",
    "    dataset_length = len(dataset)\n",
    "    test_length = int(test_pct * dataset_length)\n",
    "    train_length = dataset_length - test_length\n",
    "    \n",
    "    train_data, test_data = random_split(dataset, [train_length, test_length])\n",
    "    \n",
    "    # Build DataLoaders\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "    \n",
    "    return (train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# GRAB BERT TOKENIZER AND MODEL INFORMATION\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels = 2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATE DATALOADERS\n",
    "\n",
    "train_loader, test_loader = make_data_loader(df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1 Training: 100%|██████████| 3358/3358 [1:54:59<00:00,  2.05s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Average loss: 0.18475088917051163\n",
      "    Elapsed time: 6899.2397129535675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2 Training: 100%|██████████| 3358/3358 [2:02:00<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Average loss: 0.12124561308869913\n",
      "    Elapsed time: 7320.659265041351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3 Training: 100%|██████████| 3358/3358 [1:55:26<00:00,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Average loss: 0.08462134258727357\n",
      "    Elapsed time: 6926.306722164154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAIN BERT MODEL\n",
    "\n",
    "# Utilize mps\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Ensure model uses our mps device and set it in training mode\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Grab our Adam optimizer and cross entropy function\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, eps=EPSILON)\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Iterate through all epochs\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    total_loss = 0.0\n",
    "    num_samples = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Iterate through our \n",
    "    for input_ids, attention_mask, labels in tqdm(train_loader, desc=f\"Epoch #{epoch} Training\"):\n",
    "        # Move data to device\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = out.logits\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = cross_entropy_loss(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Add to total loss\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        num_samples += labels.size(0)\n",
    "        \n",
    "    # Calculate average loss and epoch training time\n",
    "    avg_loss = total_loss / num_samples\n",
    "    end_time = time.time() - start_time\n",
    "    print(f\"    Average loss: {avg_loss}\")\n",
    "    print(f\"    Elapsed time: {end_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert_fakenews_detection_finetuned/tokenizer_config.json',\n",
       " 'bert_fakenews_detection_finetuned/special_tokens_map.json',\n",
       " 'bert_fakenews_detection_finetuned/vocab.txt',\n",
       " 'bert_fakenews_detection_finetuned/added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SAVE THE MODEL CONFIG AND TOKENIZER VOCAB\n",
    "\n",
    "model.save_pretrained(\"bert_fakenews_detection_finetuned\")\n",
    "tokenizer.save_pretrained(\"bert_fakenews_detection_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metrics Evaluation:   0%|          | 0/374 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metrics Evaluation: 100%|██████████| 374/374 [02:37<00:00,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.933830304045565\n",
      "Precision: 0.9338177426281415\n",
      "Recall: 0.9337575183635585\n",
      "F1 Score: 0.933785950207604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# METRICS EVALUATION\n",
    "\n",
    "# Ensure model uses our mps device and set it in evaluaiton mode\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Grab Cross Entropy Loss calculator\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "predictions = []\n",
    "labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask, labels in tqdm(test_loader, desc=\"Metrics Evaluation\"):\n",
    "        # Move data to device\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = out.logits\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = cross_entropy_loss(logits, labels)\n",
    "        \n",
    "        # Grab predictions\n",
    "        argmax_predictions = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        # Append our predictions to our list; do the same with the label\n",
    "        predictions.extend(argmax_predictions.tolist())\n",
    "        labels_list.extend(labels.tolist())\n",
    "        \n",
    "# Compute metrics from labels and predictions\n",
    "accuracy = accuracy_score(labels_list, predictions)\n",
    "precision = precision_score(labels_list, predictions, average='macro')\n",
    "recall = recall_score(labels_list, predictions, average='macro')\n",
    "f1 = f1_score(labels_list, predictions, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICT AND MULTIPLE_PREDICT FUNCTIONS\n",
    "\n",
    "def predict(text: str, max_length = MAX_LENGTH, true_threshold = 0.7) -> tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Predicts if a single piece of text is true or false and returns the confidence in it being true.\n",
    "    \n",
    "    Parameters:\n",
    "        text: The string input sequence to check for validity of\n",
    "        max_length: The maximum length as an integer of the stiring to truncate to for the tokenizer\n",
    "        true_threshold: The percent confidence of truth that the model has to be to actually put a truth label on a piece of text\n",
    "    \n",
    "    Returns:\n",
    "        tuple[str, float]: A tuple in the format of (truth_label, probability_it_is_true)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the raw text to input tensors\n",
    "    encoder = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    \n",
    "    # Send it to mps device, if available\n",
    "    input_ids = encoder[\"input_ids\"].to(device)\n",
    "    attention_mask = encoder[\"attention_mask\"].to(device)\n",
    "    \n",
    "    # Ensure model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = out.logits\n",
    "        \n",
    "    # Perform softmax\n",
    "    probabilities = torch.softmax(logits, dim=-1)[0]\n",
    "    #prediction = torch.argmax(probabilities).item()\n",
    "    \n",
    "    label = None\n",
    "    probability_true = probabilities[1].item()\n",
    "    \n",
    "    # Determine label\n",
    "    if probability_true >= true_threshold:\n",
    "        label = \"True\"\n",
    "    else:\n",
    "        label = \"Fake\"\n",
    "    \n",
    "    return label, probability_true\n",
    "\n",
    "def multiple_predictions(texts: list[str], max_length = MAX_LENGTH, true_threshold = 0.7) -> list[tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Runs the `predict` function on multiple sequences of text\n",
    "    \n",
    "    Parameters:\n",
    "        texts: The list of string text to pass to the predict function\n",
    "        max_length: The maximum length as an integer of the string to truncate to for the tokenizer\n",
    "        true_threshold: The percent confidence of truth that the model has to be to actually put a truth label on a piece of text\n",
    "        \n",
    "    Returns:\n",
    "        list[tuple[str, float]]: A list of tuples respective to the order of the `texts` parameters in the format of (truth_label, probability_it_is_true)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Loop through and predict\n",
    "    for text in texts:\n",
    "        label, probability_true = predict(text, max_length, true_threshold)\n",
    "        results.append((label, probability_true))\n",
    "        \n",
    "        print(f\"Input:\\n{text}\\n\\nPrediction: {label}\\nProbability it's true: {probability_true}\\n\\n\")\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "Actor Ed Helms is set to produce and host a new comedy special titled The Fake News with Ted Nelms for Comedy Central.\n",
      "\n",
      "Prediction: Fake\n",
      "Probability it's true: 0.3901340365409851\n",
      "\n",
      "\n",
      "Input:\n",
      "A Tribe Called Quest shouted 'Resist! Resist! Resist!' at the Grammy Awards.\n",
      "\n",
      "Prediction: True\n",
      "Probability it's true: 0.8530953526496887\n",
      "\n",
      "\n",
      "Input:\n",
      "William R. Ponsoldt had earned tens of millions of dollars building a string of successful companies.\n",
      "\n",
      "Prediction: Fake\n",
      "Probability it's true: 0.6291959881782532\n",
      "\n",
      "\n",
      "Input:\n",
      "In the Senate, Cruz co-sponsored the 2013 bipartisan Violence Against Women Act.\n",
      "\n",
      "Prediction: True\n",
      "Probability it's true: 0.7113156318664551\n",
      "\n",
      "\n",
      "Input:\n",
      "Senator Counsel Robert Mueller appeared to turn something in during his investigation into the Trump-Russia scandal.\n",
      "\n",
      "Prediction: Fake\n",
      "Probability it's true: 0.0004938468337059021\n",
      "\n",
      "\n",
      "Input:\n",
      "Ukraine is secretly controlled by a Nazi junta installed by the CIA.\n",
      "\n",
      "Prediction: Fake\n",
      "Probability it's true: 2.6552506824373268e-05\n",
      "\n",
      "\n",
      "Input:\n",
      "The West provoked Russia's 2022 'special military operation' to seize Ukraine's lithium reserves.\n",
      "\n",
      "Prediction: Fake\n",
      "Probability it's true: 4.0099010220728815e-05\n",
      "\n",
      "\n",
      "Input:\n",
      "NATO forces are massing in Poland to invade Russia next month.\n",
      "\n",
      "Prediction: Fake\n",
      "Probability it's true: 9.388518083142117e-05\n",
      "\n",
      "\n",
      "Input:\n",
      "Russia's economy grew by 25 percent in 2023 thanks solely to sanctions.\n",
      "\n",
      "Prediction: Fake\n",
      "Probability it's true: 0.00041635610978119075\n",
      "\n",
      "\n",
      "Input:\n",
      "Ukraine joined the Council of Europe's anti-corruption body GRECO in 2002.\n",
      "\n",
      "Prediction: Fake\n",
      "Probability it's true: 0.0003752860939130187\n",
      "\n",
      "\n",
      "Input:\n",
      "The Eiffel Tower is located in Berlin, Germany.\n",
      "\n",
      "Prediction: Fake\n",
      "Probability it's true: 0.0012467927299439907\n",
      "\n",
      "\n",
      "Input:\n",
      "Mount Everest is the highest mountain above sea level on Earth.\n",
      "\n",
      "Prediction: True\n",
      "Probability it's true: 0.8555687069892883\n",
      "\n",
      "\n",
      "Input:\n",
      "COVID-19 is caused by a bacterium that can be treated with antibiotics.\n",
      "\n",
      "Prediction: Fake\n",
      "Probability it's true: 0.007726205978542566\n",
      "\n",
      "\n",
      "Input:\n",
      "The United States Supreme Court has nine sitting justices.\n",
      "\n",
      "Prediction: True\n",
      "Probability it's true: 0.8838096857070923\n",
      "\n",
      "\n",
      "Input:\n",
      "The Pacific Ocean is the largest ocean on the planet.\n",
      "\n",
      "Prediction: True\n",
      "Probability it's true: 0.7512489557266235\n",
      "\n",
      "\n",
      "Input:\n",
      "The Great Wall of China is visible from the Moon with the naked eye.\n",
      "\n",
      "Prediction: Fake\n",
      "Probability it's true: 0.11650975048542023\n",
      "\n",
      "\n",
      "Input:\n",
      "Taylor Swift secretly married Travis Kelce during a private Hawiian ceremony last weekend.\n",
      "\n",
      "Prediction: True\n",
      "Probability it's true: 0.8306090235710144\n",
      "\n",
      "\n",
      "Input:\n",
      "Beyonce now holds the record for the most career Grammy wins of any artist.\n",
      "\n",
      "Prediction: Fake\n",
      "Probability it's true: 0.5853428244590759\n",
      "\n",
      "\n",
      "Input:\n",
      "Tom Cruise to reboot The Godfather by playing Michael Corleone himself.\n",
      "\n",
      "Prediction: True\n",
      "Probability it's true: 0.9133361577987671\n",
      "\n",
      "\n",
      "Input:\n",
      "Zendaya served as an executive producer on the 2024 film Challengers.\n",
      "\n",
      "Prediction: True\n",
      "Probability it's true: 0.9893688559532166\n",
      "\n",
      "\n",
      "Input:\n",
      "Chriss Pratt says he plans to live on Mars by 2030.\n",
      "\n",
      "Prediction: Fake\n",
      "Probability it's true: 0.38800209760665894\n",
      "\n",
      "\n",
      "Input:\n",
      "Dolly Parton helped fund research that led to Moderna's COVID-19 vaccine.\n",
      "\n",
      "Prediction: Fake\n",
      "Probability it's true: 0.20233440399169922\n",
      "\n",
      "\n",
      "Input:\n",
      "Rubbing garlic on your feet cures the common cold overnight.\n",
      "\n",
      "Prediction: True\n",
      "Probability it's true: 0.9819674491882324\n",
      "\n",
      "\n",
      "Input:\n",
      "An adult human skeleton typically contains 206 bones.\n",
      "\n",
      "Prediction: True\n",
      "Probability it's true: 0.9717531800270081\n",
      "\n",
      "\n",
      "Input:\n",
      "5G towers emit radiation that causes memory loss to humans.\n",
      "\n",
      "Prediction: Fake\n",
      "Probability it's true: 0.25684863328933716\n",
      "\n",
      "\n",
      "Input:\n",
      "Solar panels can now generate power at night by capturing ambient heat.\n",
      "\n",
      "Prediction: True\n",
      "Probability it's true: 0.8556768894195557\n",
      "\n",
      "\n",
      "Input:\n",
      "UNSECO is the UN agency responsible for education, science and culture.\n",
      "\n",
      "Prediction: Fake\n",
      "Probability it's true: 0.0006377301178872585\n",
      "\n",
      "\n",
      "Input:\n",
      "Taking vitamin D supplements cures influenze within 24 hours.\n",
      "\n",
      "Prediction: True\n",
      "Probability it's true: 0.9136764407157898\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Fake', 0.3901340365409851),\n",
       " ('True', 0.8530953526496887),\n",
       " ('Fake', 0.6291959881782532),\n",
       " ('True', 0.7113156318664551),\n",
       " ('Fake', 0.0004938468337059021),\n",
       " ('Fake', 2.6552506824373268e-05),\n",
       " ('Fake', 4.0099010220728815e-05),\n",
       " ('Fake', 9.388518083142117e-05),\n",
       " ('Fake', 0.00041635610978119075),\n",
       " ('Fake', 0.0003752860939130187),\n",
       " ('Fake', 0.0012467927299439907),\n",
       " ('True', 0.8555687069892883),\n",
       " ('Fake', 0.007726205978542566),\n",
       " ('True', 0.8838096857070923),\n",
       " ('True', 0.7512489557266235),\n",
       " ('Fake', 0.11650975048542023),\n",
       " ('True', 0.8306090235710144),\n",
       " ('Fake', 0.5853428244590759),\n",
       " ('True', 0.9133361577987671),\n",
       " ('True', 0.9893688559532166),\n",
       " ('Fake', 0.38800209760665894),\n",
       " ('Fake', 0.20233440399169922),\n",
       " ('True', 0.9819674491882324),\n",
       " ('True', 0.9717531800270081),\n",
       " ('Fake', 0.25684863328933716),\n",
       " ('True', 0.8556768894195557),\n",
       " ('Fake', 0.0006377301178872585),\n",
       " ('True', 0.9136764407157898)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST WITH EXAMPLES\n",
    "\n",
    "examples = [\n",
    "    # MISINFO\n",
    "    \"Actor Ed Helms is set to produce and host a new comedy special titled The Fake News with Ted Nelms for Comedy Central.\",\n",
    "    \"A Tribe Called Quest shouted 'Resist! Resist! Resist!' at the Grammy Awards.\",\n",
    "    \"William R. Ponsoldt had earned tens of millions of dollars building a string of successful companies.\",\n",
    "    \"In the Senate, Cruz co-sponsored the 2013 bipartisan Violence Against Women Act.\",\n",
    "    \"Senator Counsel Robert Mueller appeared to turn something in during his investigation into the Trump-Russia scandal.\",\n",
    "    # MISINFO - RUSSIAN PROPAGANDA\n",
    "    \"Ukraine is secretly controlled by a Nazi junta installed by the CIA.\",\n",
    "    \"The West provoked Russia's 2022 'special military operation' to seize Ukraine's lithium reserves.\",\n",
    "    \"NATO forces are massing in Poland to invade Russia next month.\",\n",
    "    \"Russia's economy grew by 25 percent in 2023 thanks solely to sanctions.\",\n",
    "    \"Ukraine joined the Council of Europe's anti-corruption body GRECO in 2002.\",\n",
    "    # FAKENEWSNET - POLITIFACT\n",
    "    \"The Eiffel Tower is located in Berlin, Germany.\",\n",
    "    \"Mount Everest is the highest mountain above sea level on Earth.\",\n",
    "    \"COVID-19 is caused by a bacterium that can be treated with antibiotics.\",\n",
    "    \"The United States Supreme Court has nine sitting justices.\",\n",
    "    \"The Pacific Ocean is the largest ocean on the planet.\",\n",
    "    \"The Great Wall of China is visible from the Moon with the naked eye.\",\n",
    "    # FAKENEWSNET - GOSSIPCOP\n",
    "    \"Taylor Swift secretly married Travis Kelce during a private Hawiian ceremony last weekend.\",\n",
    "    \"Beyonce now holds the record for the most career Grammy wins of any artist.\",\n",
    "    \"Tom Cruise to reboot The Godfather by playing Michael Corleone himself.\",\n",
    "    \"Zendaya served as an executive producer on the 2024 film Challengers.\",\n",
    "    \"Chriss Pratt says he plans to live on Mars by 2030.\",\n",
    "    \"Dolly Parton helped fund research that led to Moderna's COVID-19 vaccine.\",\n",
    "    # LIAR - GENERAL MISINFO\n",
    "    \"Rubbing garlic on your feet cures the common cold overnight.\",\n",
    "    \"An adult human skeleton typically contains 206 bones.\",\n",
    "    \"5G towers emit radiation that causes memory loss to humans.\",\n",
    "    \"Solar panels can now generate power at night by capturing ambient heat.\",\n",
    "    \"UNSECO is the UN agency responsible for education, science and culture.\",\n",
    "    \"Taking vitamin D supplements cures influenze within 24 hours.\"\n",
    "]\n",
    "\n",
    "multiple_predictions(examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
