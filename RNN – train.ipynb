{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import model_utils \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZE AND CONCAT ALL DATASETS\n",
    "\n",
    "normalized_dfs = [\n",
    "    # Misinfo Dataset\n",
    "    model_utils.normalize_misinfo_dataset(\"datasets/misinfo-dataset/DataSet_Misinfo_FAKE.csv\"),\n",
    "    model_utils.normalize_misinfo_dataset(\"datasets/misinfo-dataset/DataSet_Misinfo_TRUE.csv\"),\n",
    "    model_utils.normalize_misinfo_dataset(\"datasets/misinfo-dataset/EXTRA_RussianPropagandaSubset.csv\"),\n",
    "    # Fake News Net Dataset\n",
    "    model_utils.normalize_fakenewsnet_dataset(\"datasets/fakenewsnet-dataset/gossipcop_fake.csv\"),\n",
    "    model_utils.normalize_fakenewsnet_dataset(\"datasets/fakenewsnet-dataset/gossipcop_real.csv\"),\n",
    "    model_utils.normalize_fakenewsnet_dataset(\"datasets/fakenewsnet-dataset/politifact_fake.csv\"),\n",
    "    model_utils.normalize_fakenewsnet_dataset(\"datasets/fakenewsnet-dataset/politifact_real.csv\"),\n",
    "    # Liar Dataset\n",
    "    model_utils.normalize_liar_dataset(\"datasets/liar-dataset/train.tsv\")\n",
    "]\n",
    "\n",
    "df = pd.concat(normalized_dfs, ignore_index=True)\n",
    "df = df.dropna(subset=[\"text\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are a larger number of shark attacks in Florida than there are cases of voter fraud.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[119385]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 157/157 [00:49<00:00,  3.19it/s, loss=1.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Complete. Avg Loss: 4.8003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 157/157 [00:49<00:00,  3.16it/s, loss=0.329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Complete. Avg Loss: 0.8238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 157/157 [00:52<00:00,  3.01it/s, loss=0.339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Complete. Avg Loss: 0.4549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 157/157 [00:54<00:00,  2.88it/s, loss=0.388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Complete. Avg Loss: 0.4069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 157/157 [00:52<00:00,  2.99it/s, loss=0.556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Complete. Avg Loss: 0.3892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 157/157 [00:51<00:00,  3.04it/s, loss=0.487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Complete. Avg Loss: 0.3818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 157/157 [00:50<00:00,  3.10it/s, loss=0.731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Complete. Avg Loss: 0.3743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 157/157 [00:50<00:00,  3.09it/s, loss=0.402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Complete. Avg Loss: 0.3699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 157/157 [00:49<00:00,  3.15it/s, loss=0.371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Complete. Avg Loss: 0.3639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 157/157 [00:49<00:00,  3.18it/s, loss=0.279]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Complete. Avg Loss: 0.3556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "texts = df[\"text\"].tolist()\n",
    "words = \" \".join(texts).split()\n",
    "\n",
    "vocab = sorted(set(words))\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "encoded = [word2idx[w] for w in words]\n",
    "subset = encoded[:10_000]\n",
    "\n",
    "class WordDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx : idx + self.seq_len])\n",
    "        y = torch.tensor(self.data[idx + 1 : idx + self.seq_len + 1])\n",
    "        return x, y\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True, nonlinearity=\"relu\")\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embed(x)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, hidden_size).to(device)\n",
    "\n",
    "seq_len = 10\n",
    "batch_size = 64\n",
    "embed_size = 128\n",
    "hidden_size = 256\n",
    "epochs = 10\n",
    "\n",
    "dataset = WordDataset(subset, seq_len)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "model = RNN(len(vocab), embed_size, hidden_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "    for x_batch, y_batch in loop:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        hidden = model.init_hidden(x_batch.size(0))\n",
    "\n",
    "        out, hidden = model(x_batch, hidden)\n",
    "        loss = criterion(out.view(-1, len(vocab)), y_batch.view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1} Complete. Avg Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_words, word2idx, idx2word, seq_len=10, max_new_tokens=20):\n",
    "    model.eval()\n",
    "    generated = seed_words[:]\n",
    "    input_seq = [word2idx[w] for w in seed_words[-seq_len:]]\n",
    "\n",
    "    hidden = model.init_hidden(1)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        input_tensor = torch.tensor(input_seq).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(input_tensor, hidden)\n",
    "\n",
    "        last_logits = output[0, -1]\n",
    "        probs = torch.softmax(last_logits, dim=0)\n",
    "        next_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "        next_word = idx2word[next_idx]\n",
    "\n",
    "        generated.append(next_word)\n",
    "        input_seq = input_seq[1:] + [next_idx]\n",
    "\n",
    "    return \" \".join(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the city was called out, the reference to Obama and golf: Unlike Obama, we are working to fix the golf course. However, the people s noses, because they were full of hope and expectation because of the child more than the America Despite great will is set to release this ad, called Thank\n"
     ]
    }
   ],
   "source": [
    "seed = [\"the\", \"city\", \"was\"]\n",
    "generated_text = generate_text(model, seed, word2idx, idx2word, seq_len=seq_len, max_new_tokens=50)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
