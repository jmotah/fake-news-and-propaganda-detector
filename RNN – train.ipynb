{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import model_utils \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZE AND CONCAT ALL DATASETS\n",
    "\n",
    "normalized_dfs = [\n",
    "    # Misinfo Dataset\n",
    "    model_utils.normalize_misinfo_dataset(\"datasets/misinfo-dataset/DataSet_Misinfo_FAKE.csv\"),\n",
    "    model_utils.normalize_misinfo_dataset(\"datasets/misinfo-dataset/DataSet_Misinfo_TRUE.csv\"),\n",
    "    model_utils.normalize_misinfo_dataset(\"datasets/misinfo-dataset/EXTRA_RussianPropagandaSubset.csv\"),\n",
    "    # Fake News Net Dataset\n",
    "    model_utils.normalize_fakenewsnet_dataset(\"datasets/fakenewsnet-dataset/gossipcop_fake.csv\"),\n",
    "    model_utils.normalize_fakenewsnet_dataset(\"datasets/fakenewsnet-dataset/gossipcop_real.csv\"),\n",
    "    model_utils.normalize_fakenewsnet_dataset(\"datasets/fakenewsnet-dataset/politifact_fake.csv\"),\n",
    "    model_utils.normalize_fakenewsnet_dataset(\"datasets/fakenewsnet-dataset/politifact_real.csv\"),\n",
    "    # Liar Dataset\n",
    "    model_utils.normalize_liar_dataset(\"datasets/liar-dataset/train.tsv\")\n",
    "]\n",
    "\n",
    "df = pd.concat(normalized_dfs, ignore_index=True)\n",
    "df = df.dropna(subset=[\"text\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1866/1866 [00:59<00:00, 31.57it/s, acc=0.793, loss=0.42] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Complete. Avg Loss: 0.4302, Accuracy: 0.7926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1866/1866 [00:57<00:00, 32.17it/s, acc=0.882, loss=0.346] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Complete. Avg Loss: 0.2690, Accuracy: 0.8821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1866/1866 [00:57<00:00, 32.56it/s, acc=0.914, loss=0.124] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Complete. Avg Loss: 0.1987, Accuracy: 0.9139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1866/1866 [00:56<00:00, 32.81it/s, acc=0.932, loss=0.205] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Complete. Avg Loss: 0.1617, Accuracy: 0.9323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1866/1866 [00:56<00:00, 32.80it/s, acc=0.947, loss=0.0814]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Complete. Avg Loss: 0.1276, Accuracy: 0.9472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1866/1866 [00:57<00:00, 32.34it/s, acc=0.957, loss=0.128]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Complete. Avg Loss: 0.1041, Accuracy: 0.9571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "texts = df[\"text\"].tolist()\n",
    "labels = df[\"label\"].tolist()\n",
    "words = \" \".join(texts).split()\n",
    "\n",
    "vocab = sorted(set(words))\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "\n",
    "def encode(text, seq_len):\n",
    "    tokens = text.split()\n",
    "    ids = [word2idx.get(w, 0) for w in tokens[:seq_len]]\n",
    "    return ids + [0] * (seq_len - len(ids))\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, seq_len):\n",
    "        self.data = [torch.tensor(encode(text, seq_len)) for text in texts]\n",
    "        self.labels = torch.tensor(labels).long()\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True, nonlinearity=\"relu\")\n",
    "        self.fc = nn.Linear(hidden_size, 2)\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        _, hidden = self.rnn(x)\n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "seq_len = 20\n",
    "batch_size = 64\n",
    "embed_size = 128\n",
    "hidden_size = 256\n",
    "epochs = 6\n",
    "\n",
    "# Instantiate \n",
    "dataset = ClassificationDataset(texts, labels, seq_len)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = RNNClassifier(len(vocab), embed_size, hidden_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "    for x_batch, y_batch in loop:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # predict logits\n",
    "        logits = model(x_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "        loop.set_postfix(loss=loss.item(), acc=correct / total)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Complete. Avg Loss: {total_loss / len(dataloader):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9705\n",
      "Precision: 0.9659\n",
      "Recall   : 0.9737\n",
      "F1 Score : 0.9698\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        logits = model(x_batch)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds, average=\"binary\")\n",
    "recall = recall_score(all_labels, all_preds, average=\"binary\")\n",
    "f1 = f1_score(all_labels, all_preds, average=\"binary\")\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy : {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall   : {recall:.3f}\")\n",
    "print(f\"F1 Score : {f1:.3f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
