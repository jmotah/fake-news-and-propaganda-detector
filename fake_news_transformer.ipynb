{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer – Fake News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import model_utils \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZE AND CONCAT ALL DATASETS\n",
    "\n",
    "normalized_dfs = [\n",
    "    # Misinfo Dataset\n",
    "    model_utils.normalize_misinfo_dataset(\"datasets/misinfo-dataset/DataSet_Misinfo_FAKE.csv\"),\n",
    "    model_utils.normalize_misinfo_dataset(\"datasets/misinfo-dataset/DataSet_Misinfo_TRUE.csv\"),\n",
    "    model_utils.normalize_misinfo_dataset(\"datasets/misinfo-dataset/EXTRA_RussianPropagandaSubset.csv\"),\n",
    "    # Fake News Net Dataset\n",
    "    model_utils.normalize_fakenewsnet_dataset(\"datasets/fakenewsnet-dataset/gossipcop_fake.csv\"),\n",
    "    model_utils.normalize_fakenewsnet_dataset(\"datasets/fakenewsnet-dataset/gossipcop_real.csv\"),\n",
    "    model_utils.normalize_fakenewsnet_dataset(\"datasets/fakenewsnet-dataset/politifact_fake.csv\"),\n",
    "    model_utils.normalize_fakenewsnet_dataset(\"datasets/fakenewsnet-dataset/politifact_real.csv\"),\n",
    "    # Liar Dataset\n",
    "    model_utils.normalize_liar_dataset(\"datasets/liar-dataset/train.tsv\")\n",
    "]\n",
    "\n",
    "df = pd.concat(normalized_dfs, ignore_index=True)\n",
    "df = df.dropna(subset=[\"text\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_dfs = [\n",
    "    # Misinfo Dataset\n",
    "    #model_utils.normalize_misinfo_dataset(\"datasets/misinfo-dataset/DataSet_Misinfo_FAKE.csv\"),\n",
    "    #model_utils.normalize_misinfo_dataset(\"datasets/misinfo-dataset/DataSet_Misinfo_TRUE.csv\"),\n",
    "    model_utils.normalize_misinfo_dataset(\"datasets/misinfo-dataset/EXTRA_RussianPropagandaSubset.csv\"),\n",
    "    # Fake News Net Dataset\n",
    "    #model_utils.normalize_fakenewsnet_dataset(\"datasets/fakenewsnet-dataset/gossipcop_fake.csv\"),\n",
    "    #model_utils.normalize_fakenewsnet_dataset(\"datasets/fakenewsnet-dataset/gossipcop_real.csv\"),\n",
    "    #model_utils.normalize_fakenewsnet_dataset(\"datasets/fakenewsnet-dataset/politifact_fake.csv\"),\n",
    "    #model_utils.normalize_fakenewsnet_dataset(\"datasets/fakenewsnet-dataset/politifact_real.csv\"),\n",
    "    # Liar Dataset\n",
    "    #model_utils.normalize_liar_dataset(\"datasets/liar-dataset/train.tsv\")\n",
    "]\n",
    "df = pd.concat(normalized_dfs, ignore_index=True)\n",
    "df = df.dropna(subset=[\"text\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 157/157 [00:02<00:00, 57.49it/s, loss=2.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete. Avg loss: 5.6129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 157/157 [00:02<00:00, 60.02it/s, loss=0.821]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 complete. Avg loss: 1.9418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 157/157 [00:02<00:00, 59.43it/s, loss=0.27] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 complete. Avg loss: 0.4969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 157/157 [00:02<00:00, 59.64it/s, loss=0.202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 complete. Avg loss: 0.2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 157/157 [00:02<00:00, 59.44it/s, loss=0.104] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 complete. Avg loss: 0.1191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 157/157 [00:02<00:00, 59.37it/s, loss=0.0505]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 complete. Avg loss: 0.0789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 157/157 [00:02<00:00, 59.12it/s, loss=0.0309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 complete. Avg loss: 0.0570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 157/157 [00:02<00:00, 58.89it/s, loss=0.0356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 complete. Avg loss: 0.0437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 157/157 [00:02<00:00, 59.26it/s, loss=0.0379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 complete. Avg loss: 0.0328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 157/157 [00:02<00:00, 59.02it/s, loss=0.0241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 complete. Avg loss: 0.0357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 157/157 [00:02<00:00, 59.73it/s, loss=0.0138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 complete. Avg loss: 0.0235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 157/157 [00:02<00:00, 57.88it/s, loss=0.0122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 complete. Avg loss: 0.0203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 157/157 [00:02<00:00, 58.01it/s, loss=0.0371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 complete. Avg loss: 0.0181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 157/157 [00:02<00:00, 59.11it/s, loss=0.0112]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 complete. Avg loss: 0.0239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 157/157 [00:02<00:00, 58.44it/s, loss=0.0668]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 complete. Avg loss: 0.0190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 157/157 [00:02<00:00, 59.25it/s, loss=0.00681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 complete. Avg loss: 0.0217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 157/157 [00:02<00:00, 58.39it/s, loss=0.0213] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 complete. Avg loss: 0.0170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 157/157 [00:02<00:00, 58.62it/s, loss=0.0146] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 complete. Avg loss: 0.0183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 157/157 [00:02<00:00, 58.80it/s, loss=0.0219] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 complete. Avg loss: 0.0162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 157/157 [00:02<00:00, 58.30it/s, loss=0.0444]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 complete. Avg loss: 0.0217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "texts = df[\"text\"].tolist()\n",
    "words = \" \".join(texts).split()\n",
    "\n",
    "vocab = sorted(set(words))\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "encoded = [word2idx[w] for w in words]\n",
    "subset = encoded[:10000]\n",
    "\n",
    "class WordDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx : idx + self.seq_len])\n",
    "        y = torch.tensor(self.data[idx + 1 : idx + self.seq_len + 1])\n",
    "        return x, y\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos = PositionalEncoding(d_model)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.pos(x)\n",
    "        tgt_mask = torch.triu(torch.full((x.size(1), x.size(1)), float('-inf')), diagonal=1).to(x.device)\n",
    "        out = self.decoder(x, x, tgt_mask=tgt_mask)\n",
    "        return self.fc(out)\n",
    "\n",
    "seq_len = 12\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "\n",
    "dataset = WordDataset(subset, seq_len)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = TransformerLM(len(vocab)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "    for x_batch, y_batch in loop:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        logits = model(x_batch)\n",
    "        loss = criterion(logits.view(-1, len(vocab)), y_batch.view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1} complete. Avg loss: {total_loss/len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, seed_words, word2idx, idx2word, max_new_tokens=30):\n",
    "    model.eval()\n",
    "    tokens = [word2idx[w] for w in seed_words]\n",
    "    input_ids = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        tgt = input_ids[:, -seq_len:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(tgt)\n",
    "        probs = torch.softmax(logits[0, -1], dim=0)\n",
    "        next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "        input_ids = torch.cat([input_ids, torch.tensor([[next_token]]).to(device)], dim=1)\n",
    "\n",
    "    return \" \".join([idx2word[i] for i in input_ids[0].tolist()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01: the city was shot down by a Buk anti-aircraft system belonging to Kursk's 53rd Russian anti-aircraft brigade. The Russian electoral system its Defence Agency is to maintain the narrative that\n",
      "02: in the middle the party’s loss followed through, namely history. Europe either do not think tanks and Korea. Constantinople that struggles to combat positions against Russia\". &nbsp; &nbsp; &nbsp; &nbsp;\n",
      "03: he looked at several occasions in 80 years ago are also used in the current Western front position against Russia. Ondřej Kolář, who brought up the idea of removing the\n",
      "04: she had never that never that that that that that Soviet repressions against Lithuanian partisans can be treated as genocide, the ECHR is essentially rewriting the rule of law to\n",
      "05: after all this after this declaration, the Union that after that after this after this declaration, the [local] authorities that after this declaration, the [local] authorities will make the correct\n",
      "06: when they arrived they want to involve in Romania's internal affairs. They want to bring down Ukraine, but there are some issues. The Ukrainian sailors, who are war criminals, were\n",
      "07: we decided to attempted to maintain interview is labeled “extremism”, is considered the “enemy” and is eliminated. The EU sanctions have not yet produced the desired results. The Russian Foreign\n",
      "08: it was clear was clear was clear that their heads. heads. Protesters will always unconditionally support any Russophobic regime and that Ukraine can just carry out its foreign policy and\n",
      "09: this is not is not is not happy not admit this. That Shameless curtail of Ukrainian nationalists hate because it emits extremely harmful substances into the limitation of access in\n",
      "10: nothing could have members of the European Parlament are, by the way, undeterred by this analogy. &nbsp; How could the members of the European Parliament sign a resolution, allowing the\n"
     ]
    }
   ],
   "source": [
    "seeds = [\n",
    "    [\"the\", \"city\", \"was\"],\n",
    "    [\"in\", \"the\", \"middle\"],\n",
    "    [\"he\", \"looked\", \"at\"],\n",
    "    [\"she\", \"had\", \"never\"],\n",
    "    [\"after\", \"all\", \"this\"],\n",
    "    [\"when\", \"they\", \"arrived\"],\n",
    "    [\"we\", \"decided\", \"to\"],\n",
    "    [\"it\", \"was\", \"clear\"],\n",
    "    [\"this\", \"is\", \"not\"],\n",
    "    [\"nothing\", \"could\", \"have\"]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "for i, seed in enumerate(seeds):\n",
    "    result = generate(model, seed, word2idx, idx2word, max_new_tokens=50)\n",
    "    words = result.split()\n",
    "    line = \" \".join(words[:30])\n",
    "    print(f\"{i+1:02d}: {line}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
